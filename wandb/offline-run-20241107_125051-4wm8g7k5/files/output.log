 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 135.85it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 482.99it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 136.50it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 487.86it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 135.87it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 482.96it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 140.54it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 487.52it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 137.03it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 487.05it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 136.91it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 490.65it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 137.25it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 484.91it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 137.74it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 490.40it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 137.16it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 489.97it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 136.79it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 491.51it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 137.29it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 477.90it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 138.67it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 485.73it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 136.53it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 483.95it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 136.62it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 484.95it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 138.50it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 487.07it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 139.45it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 487.09it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 136.01it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 485.17it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 136.06it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 481.94it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 137.46it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 486.72it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 136.75it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 488.27it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 232.71it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 520.09it/s]
Traceback (most recent call last):
  File "/home/lr2024/project/ResInf-main/run_micro.py", line 77, in <module>
    results = stratified_k_fold_cross_validation(args, args.fold_num, device)
  File "/home/lr2024/project/ResInf-main/real_utils.py", line 70, in stratified_k_fold_cross_validation
    final_result = train_test_faketopo(model, train_subset, test_subset, train_loader, test_loader, optimizer, criterion, args)
  File "/home/lr2024/project/ResInf-main/engine_real.py", line 138, in train_test_faketopo
    my_auc = metrics.roc_auc_score(truths, preds)
  File "/home/lr2024/.virtualenvs/ResInf-main/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 550, in roc_auc_score
    y_true = check_array(y_true, ensure_2d=False, dtype=None)
  File "/home/lr2024/.virtualenvs/ResInf-main/lib/python3.10/site-packages/sklearn/utils/validation.py", line 929, in check_array
    raise ValueError(
ValueError: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.
ResInf(
  (avg_pool): AdaptiveAvgPool2d(output_size=1)
  (max_pool): AdaptiveMaxPool2d(output_size=1)
  (sharedMLP): Sequential(
    (0): Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2d(1, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (encoder_input_layer): Linear(in_features=1, out_features=8, bias=True)
  (positional_encoding_layer): PositionalEncoder(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=8, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=8, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (gcns): ModuleList(
    (0): GCNConv(
      (linear): Linear(in_features=8, out_features=4, bias=True)
      (s_linear): Linear(in_features=8, out_features=4, bias=True)
    )
  )
  (hidden_layers): ModuleList(
    (0): Linear(in_features=4, out_features=4, bias=True)
    (1): Tanh()
  )
  (resi_net_Linear): Linear(in_features=4, out_features=3, bias=True)
  (resi_net_down): Linear(in_features=3, out_features=1, bias=True)
  (pred_linear): Linear(in_features=1, out_features=1, bias=True)
)
Namespace(lr=0.001, weight_decay=1e-05, dropout=0, hidden=2, time_tick=100, gpu=0, seed=2021, T=200.0, operator='norm_adj', epoch=50, train_size=1, valid_size=1, test_size=1, rand_guess=False, layers=1, use='start', type='node', causal=0, K=2, comment='normal', mech=1, asso=0, use_model='resinf', decompo='None', cross=0, save=1, emb_size=4, hidden_layers_num=2, pool_type='virtual', pool_arch='global', trans_layers=1, trans_emb_size=8, n_heads=1, finetune=0, train_type='kfold', threshold=0.5, fold_num=8, use_logitloss=False, use_wandb=True)
Total Loss in Epoch 0:
0.8729070796042072
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.33255016803741455, 0.33095085620880127, 0.3326992988586426, 0.33296999335289, 0.3331303596496582, 0.3316050171852112, 0.33336156606674194]
Current metric value better than 0.126984126984127 better than best -inf, saving model at /home/lr2024/project/ResInf-main/wandb/offline-run-20241107_125051-4wm8g7k5/files/checkpoints/ResInf_epoch0.pt, & logging model weights to W&B.
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
|   0   | 0.8729070796042072 | 0.9015114988599505 | 0.2857142857142857 | 0.6 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 1:
0.8506812252560441
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.346059650182724, 0.34747448563575745, 0.34686800837516785, 0.34634900093078613, 0.34663882851600647, 0.3469189703464508, 0.34668979048728943]
+-------+--------------------+-------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss     |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+--------------------+-------------------+--------------------+-----+-------------------+-----+--------------------+
|   1   | 0.8506812252560441 | 0.878683226449149 | 0.2857142857142857 | 0.3 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+--------------------+-------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 2:
0.8320102825456736
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.35807520151138306, 0.3576217293739319, 0.35784557461738586, 0.35914862155914307, 0.35917311906814575, 0.3589460849761963, 0.35909321904182434]
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
|   2   | 0.8320102825456736 | 0.8592878835541862 | 0.2857142857142857 | 0.5 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 3:
0.817137351449655
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.37109360098838806, 0.3726806342601776, 0.37081974744796753, 0.37160441279411316, 0.37280571460723877, 0.3728266954421997, 0.372292697429657]
+-------+-------------------+-------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss    |     Test Loss     |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+-------------------+-------------------+--------------------+-----+-------------------+-----+--------------------+
|   3   | 0.817137351449655 | 0.840111494064331 | 0.2857142857142857 | 0.2 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+-------------------+-------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 4:
0.8011462700610258
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.38205069303512573, 0.3840276300907135, 0.37942323088645935, 0.3835708796977997, 0.3839395344257355, 0.3833361864089966, 0.3836175203323364]
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
|   4   | 0.8011462700610258 | 0.8250644632748195 | 0.2857142857142857 | 0.0 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 5:
0.786956144838917
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.3978848159313202, 0.39814648032188416, 0.39689281582832336, 0.39798298478126526, 0.39819321036338806, 0.3981718122959137, 0.3979002833366394]
+-------+-------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss    |     Test Loss      |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+-------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
|   5   | 0.786956144838917 | 0.8035733274051121 | 0.2857142857142857 | 0.1 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+-------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 6:
0.7715528887145373
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.41152423620224, 0.41294875741004944, 0.41248103976249695, 0.4127201735973358, 0.41290733218193054, 0.4123390316963196, 0.41274431347846985]
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
|   6   | 0.7715528887145373 | 0.7849271467753819 | 0.2857142857142857 | 0.0 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 7:
0.7558974891292806
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.4284859299659729, 0.4291798174381256, 0.42865824699401855, 0.4290091097354889, 0.4290758967399597, 0.42911776900291443, 0.42906835675239563]
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
|   7   | 0.7558974891292806 | 0.7648875287600926 | 0.2857142857142857 | 0.1 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 8:
0.7416024609487883
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.4440763592720032, 0.4445187449455261, 0.4439668357372284, 0.44400131702423096, 0.4445360004901886, 0.44452139735221863, 0.4443525969982147]
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
|   8   | 0.7416024609487883 | 0.7476368291037423 | 0.2857142857142857 | 0.1 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 9:
0.7282796064201666
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.4349422752857208, 0.45575204491615295, 0.4562777578830719, 0.4559849202632904, 0.4562787711620331, 0.4555818438529968, 0.4562511742115021]
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
|   9   | 0.7282796064201666 | 0.7415788088526044 | 0.2857142857142857 | 0.3 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 10:
0.7172192943339445
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.46723756194114685, 0.4704365134239197, 0.46661287546157837, 0.4703311622142792, 0.47042304277420044, 0.47045010328292847, 0.47030532360076904]
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
|   10  | 0.7172192943339445 | 0.7224726591791425 | 0.2857142857142857 | 0.2 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 11:
0.7093291750976017
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.48045459389686584, 0.4831039011478424, 0.48311764001846313, 0.4830058515071869, 0.4831579923629761, 0.472573846578598, 0.4830634295940399]
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
|   11  | 0.7093291750976017 | 0.7121920245034354 | 0.2857142857142857 | 0.1 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 12:
0.699350685489421
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.4899994730949402, 0.49285411834716797, 0.4925016760826111, 0.4930412769317627, 0.4931361973285675, 0.4931047558784485, 0.49310940504074097]
+-------+-------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
| Epoch |     Train Loss    |     Test Loss      |      Accuracy      | AUC |         f1        | mcc |      Positive      |
+-------+-------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
|   12  | 0.699350685489421 | 0.7001854436738151 | 0.2857142857142857 | 0.3 | 0.126984126984127 | 0.0 | 0.7142857142857143 |
+-------+-------------------+--------------------+--------------------+-----+-------------------+-----+--------------------+
Total Loss in Epoch 13:
0.6806956487042564
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.5019417405128479, 0.5026349425315857, 0.5027943849563599, 0.5037314295768738, 0.5040565133094788, 0.5037102699279785, 0.5039924383163452]
Current metric value better than 0.5952380952380951 better than best 0.126984126984127, saving model at /home/lr2024/project/ResInf-main/wandb/offline-run-20241107_125051-4wm8g7k5/files/checkpoints/ResInf_epoch13.pt, & logging model weights to W&B.
Removing extra models.. [{'path': '/home/lr2024/project/ResInf-main/wandb/offline-run-20241107_125051-4wm8g7k5/files/checkpoints/ResInf_epoch0.pt', 'score': 0.126984126984127}]
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
|   13  | 0.6806956487042564 | 0.6904613716261727 | 0.7142857142857143 | 0.4 | 0.5952380952380951 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
Total Loss in Epoch 14:
0.6928198988340339
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.44877946376800537, 0.5127145648002625, 0.5108668804168701, 0.5133392810821533, 0.5133338570594788, 0.5133798122406006, 0.5133690237998962]
+-------+--------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         |         mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
|   14  | 0.6928198988340339 | 0.7017528585025242 | 0.5714285714285714 | 0.6 | 0.5194805194805194 | -0.2581988897471611 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
Total Loss in Epoch 15:
0.6799282474177224
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.5229997038841248, 0.5232717394828796, 0.5232534408569336, 0.5233016014099121, 0.5233645439147949, 0.5232030749320984, 0.5233203768730164]
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
|   15  | 0.6799282474177224 | 0.6743742908750262 | 0.7142857142857143 | 0.2 | 0.5952380952380951 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
Total Loss in Epoch 16:
0.6801193295692911
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.5301003456115723, 0.5301229953765869, 0.5296772718429565, 0.5300948619842529, 0.530120849609375, 0.5300983786582947, 0.5298857688903809]
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
|   16  | 0.6801193295692911 | 0.6693180203437805 | 0.7142857142857143 | 0.0 | 0.5952380952380951 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
Total Loss in Epoch 17:
0.6743581964045154
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.5304012298583984, 0.5371113419532776, 0.5368480682373047, 0.5370795130729675, 0.537086546421051, 0.5370993614196777, 0.5370465517044067]
+-------+--------------------+-------------------+--------------------+-----+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss     |      Accuracy      | AUC |         f1         | mcc |      Positive      |
+-------+--------------------+-------------------+--------------------+-----+--------------------+-----+--------------------+
|   17  | 0.6743581964045154 | 0.665928202016013 | 0.7142857142857143 | 0.1 | 0.5952380952380951 | 0.0 | 0.7142857142857143 |
+-------+--------------------+-------------------+--------------------+-----+--------------------+-----+--------------------+
Total Loss in Epoch 18:
0.6703133887174179
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.5437716841697693, 0.5437618494033813, 0.5437842011451721, 0.5437639355659485, 0.543815553188324, 0.543356716632843, 0.5437691807746887]
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
|   18  | 0.6703133887174179 | 0.6594980955123901 | 0.7142857142857143 | 0.4 | 0.5952380952380951 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
Total Loss in Epoch 19:
0.6722440804753985
truths [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0] preds [0.550045371055603, 0.5503120422363281, 0.5495385527610779, 0.5502185225486755, 0.5503957271575928, 0.5475286841392517, 0.5503943562507629]
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
|   19  | 0.6722440804753985 | 0.6559885910579136 | 0.7142857142857143 | 0.1 | 0.5952380952380951 | 0.0 | 0.7142857142857143 |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
Total Loss in Epoch 20:
0.2838382635797773
truths [] preds []