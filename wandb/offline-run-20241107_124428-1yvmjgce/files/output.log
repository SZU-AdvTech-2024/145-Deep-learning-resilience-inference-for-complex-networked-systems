 98%|██████████████████████████████████████████▏| 49/50 [00:01<00:00, 34.75it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 481.59it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 149.55it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 485.72it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 186.82it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 488.11it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 196.24it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 496.10it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 191.25it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 488.07it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 188.62it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 490.59it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 196.43it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 489.65it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 190.05it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 486.75it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 200.95it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 486.71it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 197.78it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 497.37it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 189.90it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 484.59it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 198.61it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 493.72it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 199.70it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 493.66it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 195.40it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 494.51it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 195.42it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 495.40it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 196.51it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 497.51it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 182.37it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 500.27it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 180.66it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 498.32it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 191.45it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 502.12it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 197.02it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 493.55it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 196.74it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 499.68it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 191.43it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 502.92it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 191.26it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 497.25it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 181.00it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 488.11it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 189.05it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 495.86it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 189.03it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 501.61it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 192.76it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 489.42it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 150.76it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 490.01it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 148.33it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 493.35it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 144.36it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 489.24it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 189.93it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 494.58it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 191.56it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 498.94it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 182.88it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 502.72it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 186.73it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 501.65it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 196.74it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 502.21it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 195.60it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 500.83it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 189.79it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 503.06it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 194.10it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 491.81it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 198.80it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 495.62it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 149.94it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 490.92it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 138.45it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 411.06it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 139.89it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 493.15it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 135.89it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 493.14it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 137.48it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 488.90it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 137.99it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 398.28it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 136.12it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 490.49it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 137.63it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 486.63it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 140.05it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 484.19it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 135.65it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 481.16it/s]
 98%|█████████████████████████████████████████▏| 49/50 [00:00<00:00, 137.44it/s]
 88%|██████████████████████████████████████▌     | 7/8 [00:00<00:00, 485.17it/s]
ResInf(
  (avg_pool): AdaptiveAvgPool2d(output_size=1)
  (max_pool): AdaptiveMaxPool2d(output_size=1)
  (sharedMLP): Sequential(
    (0): Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): ReLU()
    (2): Conv2d(1, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (encoder_input_layer): Linear(in_features=1, out_features=8, bias=True)
  (positional_encoding_layer): PositionalEncoder(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=8, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=8, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (gcns): ModuleList(
    (0): GCNConv(
      (linear): Linear(in_features=8, out_features=4, bias=True)
      (s_linear): Linear(in_features=8, out_features=4, bias=True)
    )
  )
  (hidden_layers): ModuleList(
    (0): Linear(in_features=4, out_features=4, bias=True)
    (1): Tanh()
  )
  (resi_net_Linear): Linear(in_features=4, out_features=3, bias=True)
  (resi_net_down): Linear(in_features=3, out_features=1, bias=True)
  (pred_linear): Linear(in_features=1, out_features=1, bias=True)
)
Namespace(lr=0.001, weight_decay=1e-05, dropout=0, hidden=2, time_tick=100, gpu=0, seed=2021, T=200.0, operator='norm_adj', epoch=50, train_size=1, valid_size=1, test_size=1, rand_guess=False, layers=1, use='start', type='node', causal=0, K=2, comment='normal', mech=1, asso=0, use_model='resinf', decompo='None', cross=0, save=1, emb_size=4, hidden_layers_num=2, pool_type='virtual', pool_arch='global', trans_layers=1, trans_emb_size=8, n_heads=1, finetune=0, train_type='kfold', threshold=0.5, fold_num=8, use_logitloss=False, use_wandb=True)
Total Loss in Epoch 0:
0.6399948669939625
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6141841411590576, 0.6151056885719299, 0.6155529022216797, 0.6165183186531067, 0.6169613003730774, 0.6178959012031555, 0.6132611632347107]
Current metric value better than 0.4155844155844156 better than best -inf, saving model at /home/lr2024/project/ResInf-main/wandb/offline-run-20241107_124428-1yvmjgce/files/checkpoints/ResInf_epoch0.pt, & logging model weights to W&B.
+-------+--------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss     |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   0   | 0.6399948669939625 | 0.686039251940591 | 0.5714285714285714 | 0.6666666666666667 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 1:
0.6383126566604692
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6212986707687378, 0.6163453459739685, 0.6171925663948059, 0.6198792457580566, 0.6213391423225403, 0.615873396396637, 0.6193567514419556]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   1   | 0.6383126566604692 | 0.6877591567380088 | 0.5714285714285714 | 0.5833333333333334 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 2:
0.6366824805736542
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6243531703948975, 0.6249333620071411, 0.6287242770195007, 0.6280454397201538, 0.6256090402603149, 0.6230149865150452, 0.6223908066749573]
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
|   2   | 0.6366824805736542 | 0.6852708884647915 | 0.5714285714285714 | 1.0 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
Total Loss in Epoch 3:
0.6355970958057715
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.629976749420166, 0.6317333579063416, 0.6337705850601196, 0.6326081156730652, 0.6294882297515869, 0.625609815120697, 0.6291684508323669]
+-------+--------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss     |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   3   | 0.6355970958057715 | 0.686394568000521 | 0.5714285714285714 | 0.9166666666666667 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 4:
0.6344044682930927
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6299799084663391, 0.6300191283226013, 0.6343895196914673, 0.6293403506278992, 0.6342213749885559, 0.6284695267677307, 0.6300005912780762]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   4   | 0.6344044682930927 | 0.6877305337360927 | 0.5714285714285714 | 0.8333333333333334 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 5:
0.6329553425312042
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6398825645446777, 0.6379057765007019, 0.6418960094451904, 0.6421725153923035, 0.6407138109207153, 0.6356115937232971, 0.6381787061691284]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   5   | 0.6329553425312042 | 0.6897756201880318 | 0.5714285714285714 | 0.8333333333333334 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 6:
0.6315507742823386
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6436578631401062, 0.6407603025436401, 0.6462382078170776, 0.6452990770339966, 0.6461843848228455, 0.6320589184761047, 0.6400225162506104]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   6   | 0.6315507742823386 | 0.6871155457837241 | 0.5714285714285714 | 0.9166666666666667 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 7:
0.628915724097466
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6367971897125244, 0.6399266123771667, 0.6436905264854431, 0.6491199135780334, 0.6487722396850586, 0.6344938278198242, 0.6376557350158691]
+-------+-------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
| Epoch |     Train Loss    |     Test Loss      |      Accuracy      | AUC |         f1         | mcc |      Positive      |
+-------+-------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
|   7   | 0.628915724097466 | 0.6837395642484937 | 0.5714285714285714 | 1.0 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+-------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
Total Loss in Epoch 8:
0.6248193614336909
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6529784798622131, 0.6460555195808411, 0.661110520362854, 0.6539715528488159, 0.6540018916130066, 0.6184090971946716, 0.6330112814903259]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   8   | 0.6248193614336909 | 0.6748913483960288 | 0.5714285714285714 | 0.9166666666666667 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 9:
0.619751116450952
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6430651545524597, 0.6597657799720764, 0.6667133569717407, 0.6640743613243103, 0.6610594391822815, 0.6236496567726135, 0.6364036202430725]
+-------+-------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
| Epoch |     Train Loss    |     Test Loss      |      Accuracy      | AUC |         f1         | mcc |      Positive      |
+-------+-------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
|   9   | 0.619751116450952 | 0.6662408454077584 | 0.5714285714285714 | 1.0 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+-------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
Total Loss in Epoch 10:
0.6128031623606779
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6478365659713745, 0.6266067624092102, 0.6710311770439148, 0.6674531102180481, 0.6643081307411194, 0.6168068647384644, 0.6290256381034851]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   10  | 0.6128031623606779 | 0.6677384972572327 | 0.5714285714285714 | 0.8333333333333334 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 11:
0.6006551822837518
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6400022506713867, 0.6309823393821716, 0.6820235252380371, 0.6809208393096924, 0.6615332961082458, 0.6360867619514465, 0.6410815715789795]
+-------+--------------------+--------------------+--------------------+------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC  |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+------+--------------------+-----+--------------------+
|   11  | 0.6006551822837518 | 0.6711185915129525 | 0.5714285714285714 | 0.75 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+------+--------------------+-----+--------------------+
Total Loss in Epoch 12:
0.5980547453675952
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6935681700706482, 0.6238520741462708, 0.699213981628418, 0.6999014616012573, 0.6981251239776611, 0.6369336843490601, 0.6903067231178284]
+-------+--------------------+--------------------+--------------------+------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC  |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+------+--------------------+-----+--------------------+
|   12  | 0.5980547453675952 | 0.7019879690238408 | 0.5714285714285714 | 0.75 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+------+--------------------+-----+--------------------+
Total Loss in Epoch 13:
0.596263415351206
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.665599524974823, 0.6515921950340271, 0.7055392265319824, 0.7021999955177307, 0.7026867270469666, 0.5946630835533142, 0.6498692035675049]
+-------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss    |     Test Loss      |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   13  | 0.596263415351206 | 0.6473445040839059 | 0.5714285714285714 | 0.9166666666666667 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 14:
0.5916954035661659
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6396776437759399, 0.6093613505363464, 0.7132423520088196, 0.7123132944107056, 0.695000946521759, 0.5793256759643555, 0.5900722742080688]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   14  | 0.5916954035661659 | 0.6163977682590485 | 0.5714285714285714 | 0.9166666666666667 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 15:
0.5728587897456422
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7028881311416626, 0.5990222692489624, 0.7225063443183899, 0.7127293944358826, 0.7075514793395996, 0.5627146363258362, 0.617404580116272]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   15  | 0.5728587897456422 | 0.6462395659514836 | 0.5714285714285714 | 0.8333333333333334 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 16:
0.558484144356786
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.687481164932251, 0.6208783388137817, 0.7321452498435974, 0.7334961891174316, 0.730093777179718, 0.5529888272285461, 0.6800112128257751]
+-------+-------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss    |     Test Loss     |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+-------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   16  | 0.558484144356786 | 0.645806189094271 | 0.5714285714285714 | 0.8333333333333334 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+-------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 17:
0.5463595293006118
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6890655159950256, 0.6696523427963257, 0.7446863651275635, 0.7442487478256226, 0.7257174849510193, 0.5474748611450195, 0.6246854662895203]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   17  | 0.5463595293006118 | 0.6075484369482312 | 0.5714285714285714 | 0.9166666666666667 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 18:
0.5408825989888639
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7264830470085144, 0.6062706708908081, 0.7523223161697388, 0.7495625019073486, 0.7424226403236389, 0.5241101980209351, 0.5643935203552246]
+-------+--------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss     |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+--------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   18  | 0.5408825989888639 | 0.605871149471828 | 0.5714285714285714 | 0.9166666666666667 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+-------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 19:
0.5243714543021455
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.595641553401947, 0.6170693039894104, 0.7602519989013672, 0.760968029499054, 0.7518513798713684, 0.519255518913269, 0.5777910351753235]
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
|   19  | 0.5243714543021455 | 0.5450553808893476 | 0.5714285714285714 | 1.0 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
Total Loss in Epoch 20:
0.518891340615798
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7127649188041687, 0.5940515398979187, 0.767890214920044, 0.7682384252548218, 0.759978175163269, 0.5005570650100708, 0.6161248087882996]
+-------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
| Epoch |     Train Loss    |     Test Loss      |      Accuracy      |        AUC         |         f1         | mcc |      Positive      |
+-------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
|   20  | 0.518891340615798 | 0.6031674912997654 | 0.5714285714285714 | 0.8333333333333334 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+
Total Loss in Epoch 21:
0.5398112091482902
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.709736168384552, 0.6773524284362793, 0.7730156779289246, 0.7733649015426636, 0.7715165615081787, 0.4869507849216461, 0.6684411764144897]
Current metric value better than 0.6714285714285715 better than best 0.4155844155844156, saving model at /home/lr2024/project/ResInf-main/wandb/offline-run-20241107_124428-1yvmjgce/files/checkpoints/ResInf_epoch21.pt, & logging model weights to W&B.
Removing extra models.. [{'path': '/home/lr2024/project/ResInf-main/wandb/offline-run-20241107_124428-1yvmjgce/files/checkpoints/ResInf_epoch0.pt', 'score': 0.4155844155844156}]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   21  | 0.5398112091482902 | 0.5959599145821163 | 0.7142857142857143 | 0.9166666666666667 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 22:
0.5423014845166888
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7515688538551331, 0.532646894454956, 0.7770496606826782, 0.7765511274337769, 0.7708118557929993, 0.4831380546092987, 0.7199375033378601]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   22  | 0.5423014845166888 | 0.6743804131235395 | 0.7142857142857143 | 0.8333333333333334 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 23:
0.5171711055599914
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6137311458587646, 0.6251122355461121, 0.782187819480896, 0.7823178172111511, 0.741487979888916, 0.4862540662288666, 0.5352838039398193]
+-------+--------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss     |      Accuracy      | AUC |         f1         |         mcc         |      Positive      |
+-------+--------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
|   23  | 0.5171711055599914 | 0.520521634391376 | 0.7142857142857143 | 1.0 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
Total Loss in Epoch 24:
0.5070960384850599
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7704082131385803, 0.7801890969276428, 0.7878528237342834, 0.7878378629684448, 0.7805859446525574, 0.5031207203865051, 0.5134204030036926]
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         | mcc |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
|   24  | 0.5070960384850599 | 0.5520073814051492 | 0.5714285714285714 | 1.0 | 0.4155844155844156 | 0.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+-----+--------------------+-----+--------------------+
Total Loss in Epoch 25:
0.5066660438873329
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7564355731010437, 0.7482609152793884, 0.7918741106987, 0.7917116284370422, 0.7897744178771973, 0.48214206099510193, 0.6810381412506104]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   25  | 0.5066660438873329 | 0.6008620368582862 | 0.7142857142857143 | 0.9166666666666667 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 26:
0.4998744057149303
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7687221765518188, 0.6862229108810425, 0.7960297465324402, 0.7960196137428284, 0.7940630316734314, 0.461101233959198, 0.6632349491119385]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   26  | 0.4998744057149303 | 0.6048754900693893 | 0.7142857142857143 | 0.9166666666666667 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 27:
0.48909066769541526
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7963249087333679, 0.7972304821014404, 0.8002565503120422, 0.7993016839027405, 0.7980424165725708, 0.4558764696121216, 0.7631398439407349]
+-------+---------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
| Epoch |      Train Loss     |     Test Loss     |      Accuracy      | AUC |         f1         |         mcc         |      Positive      |
+-------+---------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
|   27  | 0.48909066769541526 | 0.648448320371764 | 0.7142857142857143 | 1.0 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+---------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
Total Loss in Epoch 28:
0.4960425882315149
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.5180063843727112, 0.7365577816963196, 0.8038479089736938, 0.8035579323768616, 0.8017076849937439, 0.46986860036849976, 0.5538135170936584]
+-------+--------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss     |      Accuracy      | AUC |         f1         |         mcc         |      Positive      |
+-------+--------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
|   28  | 0.4960425882315149 | 0.447900442140443 | 0.7142857142857143 | 1.0 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
Total Loss in Epoch 29:
0.49636784287131563
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7682991027832031, 0.7982456088066101, 0.8062767386436462, 0.806248664855957, 0.805927574634552, 0.47534605860710144, 0.656507670879364]
+-------+---------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
| Epoch |      Train Loss     |     Test Loss     |      Accuracy      | AUC |         f1         |         mcc         |      Positive      |
+-------+---------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
|   29  | 0.49636784287131563 | 0.578243783542088 | 0.7142857142857143 | 1.0 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+---------------------+-------------------+--------------------+-----+--------------------+---------------------+--------------------+
Total Loss in Epoch 30:
0.49650702975234207
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6866404414176941, 0.6867383122444153, 0.8102417588233948, 0.810225784778595, 0.8081399202346802, 0.4400581121444702, 0.5526580214500427]
+-------+---------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
| Epoch |      Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         |         mcc         |      Positive      |
+-------+---------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
|   30  | 0.49650702975234207 | 0.5077778739588601 | 0.7142857142857143 | 1.0 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+---------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
Total Loss in Epoch 31:
0.4655051736199126
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7757524847984314, 0.7342137098312378, 0.8126526474952698, 0.8124920725822449, 0.8120267987251282, 0.4590851664543152, 0.7103565335273743]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   31  | 0.4655051736199126 | 0.6115543629441943 | 0.7142857142857143 | 0.9166666666666667 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 32:
0.491009030110982
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7577739953994751, 0.7574309706687927, 0.8154054284095764, 0.8154985308647156, 0.814813494682312, 0.4626694321632385, 0.6968756914138794]
+-------+-------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |     Train Loss    |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+-------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   32  | 0.491009030110982 | 0.5890403794390815 | 0.7142857142857143 | 0.9166666666666667 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+-------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 33:
0.46046399401158705
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.8181172609329224, 0.631629467010498, 0.8185890913009644, 0.8186394572257996, 0.8157790303230286, 0.41728121042251587, 0.577490508556366]
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |      Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   33  | 0.46046399401158705 | 0.5956193889890399 | 0.7142857142857143 | 0.8333333333333334 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 34:
0.46160403228535946
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.8175946474075317, 0.8050783276557922, 0.8218985795974731, 0.8219076991081238, 0.8209044933319092, 0.44150641560554504, 0.7664755582809448]
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |      Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   34  | 0.46160403228535946 | 0.6492763140371868 | 0.7142857142857143 | 0.9166666666666667 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 35:
0.47975006182583013
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7601354718208313, 0.7033888101577759, 0.8236119747161865, 0.8235815763473511, 0.8232505917549133, 0.4096042215824127, 0.7730512619018555]
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |      Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   35  | 0.47975006182583013 | 0.6245946905442646 | 0.7142857142857143 | 0.8333333333333334 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 36:
0.4595853248421027
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.760761559009552, 0.8116793036460876, 0.8265482187271118, 0.8265605568885803, 0.8261837959289551, 0.4325513541698456, 0.8069219589233398]
+-------+--------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         |         mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
|   36  | 0.4595853248421027 | 0.6317324957677296 | 0.7142857142857143 | 1.0 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
Total Loss in Epoch 37:
0.4717667504232757
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.6183558106422424, 0.8260481953620911, 0.8282529711723328, 0.8282504677772522, 0.826969563961029, 0.3971746861934662, 0.6395019292831421]
+-------+--------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         |         mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
|   37  | 0.4717667504232757 | 0.4639469917331423 | 0.7142857142857143 | 1.0 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
Total Loss in Epoch 38:
0.45065033405411
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.8292092680931091, 0.7831026315689087, 0.8310232758522034, 0.8309956192970276, 0.8305342197418213, 0.4259132146835327, 0.7839305400848389]
+-------+------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |    Train Loss    |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   38  | 0.45065033405411 | 0.6649789001260485 | 0.7142857142857143 | 0.8333333333333334 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 39:
0.4633543397090873
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.8067924380302429, 0.7942855954170227, 0.832537055015564, 0.8323915600776672, 0.8320662379264832, 0.39637842774391174, 0.7507697343826294]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   39  | 0.4633543397090873 | 0.6170087818588529 | 0.7142857142857143 | 0.9166666666666667 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 40:
0.43915727825797335
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.7572776675224304, 0.7883432507514954, 0.8351731896400452, 0.8350573182106018, 0.8349800705909729, 0.3888990879058838, 0.6971883773803711]
+-------+---------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
| Epoch |      Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         |         mcc         |      Positive      |
+-------+---------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
|   40  | 0.43915727825797335 | 0.5545021231685366 | 0.7142857142857143 | 1.0 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+---------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
Total Loss in Epoch 41:
0.43497124223076566
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.8359338641166687, 0.6801682114601135, 0.8378342986106873, 0.8377887010574341, 0.8369850516319275, 0.3754693269729614, 0.5030682682991028]
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |      Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   41  | 0.43497124223076566 | 0.5564045203583581 | 0.7142857142857143 | 0.9166666666666667 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 42:
0.4648275068219827
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.837916374206543, 0.8371481895446777, 0.8393399715423584, 0.8393646478652954, 0.8385742902755737, 0.3762444257736206, 0.518872857093811]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   42  | 0.4648275068219827 | 0.5324741538081851 | 0.7142857142857143 | 0.9166666666666667 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 43:
0.465555375023764
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.514544665813446, 0.7203714847564697, 0.8405080437660217, 0.8404861092567444, 0.8400679230690002, 0.37153494358062744, 0.43617671728134155]
Current metric value better than 0.8507936507936508 better than best 0.6714285714285715, saving model at /home/lr2024/project/ResInf-main/wandb/offline-run-20241107_124428-1yvmjgce/files/checkpoints/ResInf_epoch43.pt, & logging model weights to W&B.
Removing extra models.. [{'path': '/home/lr2024/project/ResInf-main/wandb/offline-run-20241107_124428-1yvmjgce/files/checkpoints/ResInf_epoch21.pt', 'score': 0.6714285714285715}]
+-------+-------------------+---------------------+--------------------+-----+--------------------+--------------------+--------------------+
| Epoch |     Train Loss    |      Test Loss      |      Accuracy      | AUC |         f1         |        mcc         |      Positive      |
+-------+-------------------+---------------------+--------------------+-----+--------------------+--------------------+--------------------+
|   43  | 0.465555375023764 | 0.37284880237919943 | 0.8571428571428571 | 1.0 | 0.8507936507936508 | 0.7302967433402214 | 0.5714285714285714 |
+-------+-------------------+---------------------+--------------------+-----+--------------------+--------------------+--------------------+
Total Loss in Epoch 44:
0.4449932702950069
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.84002685546875, 0.7927495241165161, 0.8427073955535889, 0.8426598310470581, 0.8422293663024902, 0.3710942566394806, 0.6388294696807861]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   44  | 0.4449932702950069 | 0.5801723748445511 | 0.7142857142857143 | 0.9166666666666667 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 45:
0.4314126071273064
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.837337851524353, 0.7055351138114929, 0.8447285890579224, 0.844736635684967, 0.8444668650627136, 0.3800273537635803, 0.4841228723526001]
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
| Epoch |     Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |        mcc         |      Positive      |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
|   45  | 0.4314126071273064 | 0.5444807708263397 | 0.8571428571428571 | 0.9166666666666667 | 0.8507936507936508 | 0.7302967433402214 | 0.5714285714285714 |
+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
Total Loss in Epoch 46:
0.46793125873925734
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.8445862531661987, 0.8136147260665894, 0.8460266590118408, 0.8460049033164978, 0.8456615209579468, 0.4197278618812561, 0.6173567175865173]
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
| Epoch |      Train Loss     |     Test Loss      |      Accuracy      |        AUC         |         f1         |         mcc         |      Positive      |
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
|   46  | 0.46793125873925734 | 0.5821305172783988 | 0.7142857142857143 | 0.9166666666666667 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+---------------------+--------------------+--------------------+--------------------+--------------------+---------------------+--------------------+
Total Loss in Epoch 47:
0.43993822378771646
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.8396092057228088, 0.8399168252944946, 0.8475356698036194, 0.8475572466850281, 0.8470407724380493, 0.3635185956954956, 0.7854533791542053]
+-------+---------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
| Epoch |      Train Loss     |     Test Loss      |      Accuracy      | AUC |         f1         |         mcc         |      Positive      |
+-------+---------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
|   47  | 0.43993822378771646 | 0.6417783434901919 | 0.7142857142857143 | 1.0 | 0.6714285714285715 | 0.47140452079103173 | 0.5714285714285714 |
+-------+---------------------+--------------------+--------------------+-----+--------------------+---------------------+--------------------+
Total Loss in Epoch 48:
0.4823218039712127
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.49016696214675903, 0.7865016460418701, 0.8488302230834961, 0.8488163948059082, 0.8477836847305298, 0.3573324382305145, 0.4605158865451813]
Current metric value better than 1.0 better than best 0.8507936507936508, saving model at /home/lr2024/project/ResInf-main/wandb/offline-run-20241107_124428-1yvmjgce/files/checkpoints/ResInf_epoch48.pt, & logging model weights to W&B.
Removing extra models.. [{'path': '/home/lr2024/project/ResInf-main/wandb/offline-run-20241107_124428-1yvmjgce/files/checkpoints/ResInf_epoch43.pt', 'score': 0.8507936507936508}]
+-------+--------------------+--------------------+----------+-----+-----+-----+--------------------+
| Epoch |     Train Loss     |     Test Loss      | Accuracy | AUC |  f1 | mcc |      Positive      |
+-------+--------------------+--------------------+----------+-----+-----+-----+--------------------+
|   48  | 0.4823218039712127 | 0.3522914733205523 |   1.0    | 1.0 | 1.0 | 1.0 | 0.5714285714285714 |
+-------+--------------------+--------------------+----------+-----+-----+-----+--------------------+
Total Loss in Epoch 49:
0.4432738633180151
truths [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0] preds [0.4695591330528259, 0.8449816107749939, 0.8503005504608154, 0.8503022789955139, 0.8500292897224426, 0.3563218116760254, 0.6306318044662476]
+-------+--------------------+---------------------+--------------------+-----+--------------------+--------------------+--------------------+
| Epoch |     Train Loss     |      Test Loss      |      Accuracy      | AUC |         f1         |        mcc         |      Positive      |
+-------+--------------------+---------------------+--------------------+-----+--------------------+--------------------+--------------------+
|   49  | 0.4432738633180151 | 0.38940259175641195 | 0.8571428571428571 | 1.0 | 0.8507936507936508 | 0.7302967433402214 | 0.5714285714285714 |
+-------+--------------------+---------------------+--------------------+-----+--------------------+--------------------+--------------------+